{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1970705,
     "status": "ok",
     "timestamp": 1524577821923,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg",
      "userId": "116292885805316472049"
     },
     "user_tz": -330
    },
    "id": "H9EU0e8yzFOm",
    "outputId": "b9a7331d-3a52-4991-f4a2-eaa64e288eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 17:59:04.471589  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0811 17:59:04.487215  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0811 17:59:04.487215  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0811 17:59:04.643455  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0811 17:59:04.643455  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0811 17:59:04.659076  7896 deprecation.py:506] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0811 17:59:04.752821  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 17:59:04.768444  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0811 17:59:04.877831  7896 deprecation.py:323] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.2674 - acc: 0.9171 - val_loss: 0.0611 - val_acc: 0.9786\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0871 - acc: 0.9742 - val_loss: 0.0517 - val_acc: 0.9828\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0658 - acc: 0.9798 - val_loss: 0.0360 - val_acc: 0.9883\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.0521 - acc: 0.9847 - val_loss: 0.0349 - val_acc: 0.9887\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0454 - acc: 0.9863 - val_loss: 0.0309 - val_acc: 0.9896\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0418 - acc: 0.9874 - val_loss: 0.0261 - val_acc: 0.9917\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0366 - acc: 0.9887 - val_loss: 0.0286 - val_acc: 0.9907\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0325 - acc: 0.9899 - val_loss: 0.0288 - val_acc: 0.9912\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0302 - acc: 0.9903 - val_loss: 0.0275 - val_acc: 0.9913\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0273 - acc: 0.9917 - val_loss: 0.0307 - val_acc: 0.9915\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0279 - acc: 0.9915 - val_loss: 0.0265 - val_acc: 0.9921\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0283 - val_acc: 0.9924\n",
      "Test loss: 0.028258230628092677\n",
      "Test accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make about 3 distinct architectures of convolutional netwroks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 18:35:14.560059  7896 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.1829 - acc: 0.9419 - val_loss: 0.0568 - val_acc: 0.9823\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0683 - acc: 0.9788 - val_loss: 0.0425 - val_acc: 0.9857\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0523 - acc: 0.9841 - val_loss: 0.0399 - val_acc: 0.9878\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0447 - acc: 0.9857 - val_loss: 0.0315 - val_acc: 0.9904\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0367 - acc: 0.9883 - val_loss: 0.0325 - val_acc: 0.9895\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.0325 - acc: 0.9894 - val_loss: 0.0351 - val_acc: 0.9887\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.0302 - acc: 0.9902 - val_loss: 0.0338 - val_acc: 0.9901\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0279 - acc: 0.9908 - val_loss: 0.0296 - val_acc: 0.9905\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0244 - acc: 0.9922 - val_loss: 0.0306 - val_acc: 0.9908\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0221 - acc: 0.9928 - val_loss: 0.0278 - val_acc: 0.9921\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0258 - val_acc: 0.9922\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0205 - acc: 0.9933 - val_loss: 0.0283 - val_acc: 0.9919\n",
      "Test loss: 0.028312752436022673\n",
      "Test accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model_1.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model_1.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(64, activation='relu'))\n",
    "model_1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_1 = model_1.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_1[0])\n",
    "print('Test accuracy:', score_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arhcitecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 303s 5ms/step - loss: 0.1254 - acc: 0.9651 - val_loss: 0.0680 - val_acc: 0.9776\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 303s 5ms/step - loss: 0.0431 - acc: 0.9879 - val_loss: 0.0437 - val_acc: 0.9840\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 302s 5ms/step - loss: 0.0320 - acc: 0.9904 - val_loss: 0.0273 - val_acc: 0.9919\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 300s 5ms/step - loss: 0.0230 - acc: 0.9928 - val_loss: 0.0477 - val_acc: 0.9852\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 305s 5ms/step - loss: 0.0198 - acc: 0.9941 - val_loss: 0.0386 - val_acc: 0.9867\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 328s 5ms/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.0397 - val_acc: 0.9879\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 332s 6ms/step - loss: 0.0143 - acc: 0.9953 - val_loss: 0.0364 - val_acc: 0.9881\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 317s 5ms/step - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0234 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 325s 5ms/step - loss: 0.0105 - acc: 0.9966 - val_loss: 0.0254 - val_acc: 0.9925\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 311s 5ms/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0317 - val_acc: 0.9905\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 311s 5ms/step - loss: 0.0101 - acc: 0.9966 - val_loss: 0.0365 - val_acc: 0.9900\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 315s 5ms/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0322 - val_acc: 0.9913\n",
      "Test loss: 0.03223905730191964\n",
      "Test accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model_2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_1.add(Dropout(0.1))\n",
    "model_2.add(Conv2D(64, (2, 2), activation='relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(64, activation='relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Dropout(0.25))\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_2 = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_2[0])\n",
    "print('Test accuracy:', score_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 316s 5ms/step - loss: 0.1750 - acc: 0.9469 - val_loss: 0.4786 - val_acc: 0.8493\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 319s 5ms/step - loss: 0.0886 - acc: 0.9730 - val_loss: 2.1779 - val_acc: 0.6131\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 317s 5ms/step - loss: 0.0738 - acc: 0.9772 - val_loss: 0.1461 - val_acc: 0.9577\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 327s 5ms/step - loss: 0.0647 - acc: 0.9802 - val_loss: 0.5562 - val_acc: 0.8509\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 326s 5ms/step - loss: 0.0578 - acc: 0.9822 - val_loss: 0.1194 - val_acc: 0.9633\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 324s 5ms/step - loss: 0.0552 - acc: 0.9826 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 324s 5ms/step - loss: 0.0515 - acc: 0.9846 - val_loss: 0.2179 - val_acc: 0.9389\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 323s 5ms/step - loss: 0.0474 - acc: 0.9854 - val_loss: 0.1628 - val_acc: 0.9502\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 323s 5ms/step - loss: 0.0452 - acc: 0.9857 - val_loss: 0.1405 - val_acc: 0.9575\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 319s 5ms/step - loss: 0.0427 - acc: 0.9868 - val_loss: 0.0781 - val_acc: 0.9750\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 318s 5ms/step - loss: 0.0407 - acc: 0.9878 - val_loss: 0.1738 - val_acc: 0.9424\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 332s 6ms/step - loss: 0.0381 - acc: 0.9885 - val_loss: 0.0346 - val_acc: 0.9881\n",
      "Test loss: 0.03459447893754113\n",
      "Test accuracy: 0.9881\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Conv2D(32, kernel_size=(2, 2),\n",
    "                 activation='sigmoid',\n",
    "                 input_shape=input_shape))\n",
    "model_3.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(128, activation='sigmoid'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adagrad(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_3 = model_3.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_3[0])\n",
    "print('Test accuracy:', score_3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 22:37:34.260540  6576 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0811 22:37:34.260540  6576 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0811 22:37:34.338661  6576 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0811 22:37:34.416781  6576 deprecation.py:506] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0811 22:37:34.432404  6576 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0811 22:37:34.573019  6576 deprecation_wrapper.py:119] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 22:37:34.682407  6576 deprecation.py:323] From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 354s 6ms/step - loss: 0.1780 - acc: 0.9479 - val_loss: 0.0673 - val_acc: 0.9795\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 378s 6ms/step - loss: 0.0719 - acc: 0.9778 - val_loss: 0.0656 - val_acc: 0.9792\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 360s 6ms/step - loss: 0.0588 - acc: 0.9825 - val_loss: 0.0538 - val_acc: 0.9835\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 368s 6ms/step - loss: 0.0494 - acc: 0.9842 - val_loss: 0.0468 - val_acc: 0.9853\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 366s 6ms/step - loss: 0.0438 - acc: 0.9871 - val_loss: 0.0588 - val_acc: 0.9811\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 362s 6ms/step - loss: 0.0389 - acc: 0.9884 - val_loss: 0.0463 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 397s 7ms/step - loss: 0.0340 - acc: 0.9893 - val_loss: 0.0412 - val_acc: 0.9869\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 404s 7ms/step - loss: 0.0320 - acc: 0.9901 - val_loss: 0.0568 - val_acc: 0.9814\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 387s 6ms/step - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0472 - val_acc: 0.9851\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 381s 6ms/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0364 - val_acc: 0.9884\n",
      "Test loss: 0.036352840721810935\n",
      "Test accuracy: 0.9884\n"
     ]
    }
   ],
   "source": [
    "# Activation = selu & softmax , without Batch normalization and dropout, Initializer = kernel_initializer &\n",
    "# bias_initializer, Optimizer = Adagrad\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape))\n",
    "model_4.add(Dense(45,\n",
    "                kernel_initializer='random_uniform'))\n",
    "model_4.add(Conv2D(64, (4, 4), activation='softmax'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dropout(0.25))\n",
    "model_4.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(128, activation='sigmoid'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_4.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adamax(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_4.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_4 = model_4.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_4[0])\n",
    "print('Test accuracy:', score_4[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----------+--------------+----------------------+---------------+\n",
      "| Sr.No. |     Model      | Optimizer | Activation   |      Test Loss       | Test Accuracy |\n",
      "+--------+----------------+-----------+--------------+----------------------+---------------+\n",
      "|   1    | Architecture 1 |  Adadelta |     relu     | 0.028312752436022673 |     0.9919    |\n",
      "|   2    | Architecture 2 |    Adam   |     relu     | 0.03223905730191964  |     0.9913    |\n",
      "|   3    | Architecture 3 |  Adagrad  |   sigmoid    | 0.03459447893754113  |     0.9881    |\n",
      "|   4    | Architecture 4 |   Adamax  | tanh,sigmoid | 0.036352840721810935 |     0.9884    |\n",
      "+--------+----------------+-----------+--------------+----------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "number= [1,2,3,4] \n",
    "model = [\"Architecture 1\",\"Architecture 2\",\"Architecture 3\",\"Architecture 4\"] \n",
    "opt   = ['Adadelta', 'Adam', 'Adagrad', 'Adamax']\n",
    "act   = ['relu', 'relu', 'sigmoid', 'tanh,sigmoid']\n",
    "loss  = [0.028312752436022673,0.03223905730191964,0.03459447893754113,score_4[0]] \n",
    "acc   = [0.9919,0.9913,0.9881,score_4[1]]       \n",
    "\n",
    "#Initialize Prettytable \n",
    "pt = PrettyTable() \n",
    "pt.add_column(\"Sr.No.\", number) \n",
    "pt.add_column(\"Model\", model) \n",
    "pt.add_column(\"Optimizer\", opt)\n",
    "pt.add_column('Activation ', act)\n",
    "pt.add_column(\"Test Loss\", loss) \n",
    "pt.add_column(\"Test Accuracy\", acc) \n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture 1 is getting highest accuracy of 99.19% with minimum loss, in which I have used Adadelta optimizer and relu activation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have tried 4 CNN architectures:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. with differnt number of layers\n",
    "### 2. different kernel values: (2,2), (3,3), (5,5)\n",
    "### 3. different activations: relu, tanh, sigmoid\n",
    "### 4. different optimizers: adam, adadelta, adagrad, adamax\n",
    "### 5. adding batchnormalisation layers\n",
    "### 6. adding some dropout layers, maxpool layers, flattening layers and dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
